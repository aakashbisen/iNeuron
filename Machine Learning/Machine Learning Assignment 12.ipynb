{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "111120c6",
   "metadata": {
    "id": "111120c6"
   },
   "source": [
    "##### 1. What is prior probability ? Give an example ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104d6217",
   "metadata": {
    "id": "104d6217"
   },
   "source": [
    "**Ans:**\n",
    "Prior probability refers to the initial probability assigned to an event or hypothesis before any evidence or data is taken into account. It represents the belief or probability assigned based on prior knowledge or assumptions.\n",
    "\n",
    "Here's an example to illustrate prior probability:\n",
    "\n",
    "Let's consider a medical scenario. Suppose there is a rare disease that affects 1 in 10,000 people in a population. Before conducting any tests or examining specific symptoms, the prior probability of an individual in the population having the disease is 1/10,000 or 0.01%.\n",
    "\n",
    "In this case, the prior probability is based on general knowledge or assumptions about the disease prevalence in the population. It represents the initial belief about the likelihood of an individual having the disease without considering any additional information.\n",
    "\n",
    "Prior probabilities serve as a starting point in statistical inference and Bayesian analysis. They can be updated using new evidence or data to obtain posterior probabilities that reflect the updated belief or probability based on both prior knowledge and the observed information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae857d4",
   "metadata": {
    "id": "4ae857d4"
   },
   "source": [
    "##### 2. What is posterior probability ? Give an example ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c74bcef",
   "metadata": {
    "id": "9c74bcef"
   },
   "source": [
    "**Ans:**\n",
    "Posterior probability refers to the updated probability of an event or hypothesis after taking into account new evidence or data. It is derived by combining the prior probability with the likelihood of the observed data given the event or hypothesis.\n",
    "\n",
    "Here's an example to illustrate posterior probability:\n",
    "\n",
    "Suppose we have a diagnostic test for a disease with a known sensitivity of 90% (the probability of a positive test result given that the individual has the disease) and a specificity of 95% (the probability of a negative test result given that the individual does not have the disease). Let's assume the prior probability of an individual having the disease is 0.01%.\n",
    "\n",
    "Now, consider an individual who undergoes the diagnostic test and receives a positive result. We want to calculate the posterior probability of the individual having the disease given the positive test result.\n",
    "\n",
    "To do this, we can use Bayes' theorem, which states:\n",
    "\n",
    "Posterior probability = (Prior probability * Likelihood) / Evidence\n",
    "\n",
    "Prior probability: 0.01% (prior belief or probability based on general knowledge)\n",
    "Likelihood: Probability of a positive test result given the disease (sensitivity) = 90%\n",
    "Evidence: Probability of a positive test result irrespective of the disease\n",
    "\n",
    "To calculate the evidence, we need to consider both true positive and false positive scenarios:\n",
    "\n",
    "Evidence = (Prior probability * Sensitivity) + ((1 - Prior probability) * (1 - Specificity))\n",
    "\n",
    "Plugging in the values:\n",
    "\n",
    "Evidence = (0.0001 * 0.9) + (0.9999 * 0.05) ≈ 0.05009\n",
    "\n",
    "Finally, we can calculate the posterior probability:\n",
    "\n",
    "Posterior probability = (Prior probability * Likelihood) / Evidence\n",
    "= (0.0001 * 0.9) / 0.05009 ≈ 0.001799\n",
    "\n",
    "Therefore, the posterior probability of the individual having the disease given the positive test result is approximately 0.001799 or 0.1799%.\n",
    "\n",
    "The posterior probability reflects the updated probability based on both the prior belief and the new evidence provided by the positive test result. It incorporates the sensitivity and specificity of the diagnostic test to provide a more accurate estimation of the probability of the individual having the disease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c981ac1e",
   "metadata": {
    "id": "c981ac1e"
   },
   "source": [
    "##### 3. What is likelihood probability ? Give an example ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8618f0f7",
   "metadata": {
    "id": "8618f0f7"
   },
   "source": [
    "**Ans:**\n",
    "The term \"likelihood probability\" is a common term used to describe the likelihood function or the probability distribution of the observed data given the parameters of a statistical model. It represents how likely the observed data is under a specific set of parameter values.\n",
    "\n",
    "Here's an example to illustrate likelihood probability:\n",
    "\n",
    "Let's consider a simple example of flipping a fair coin. The coin can result in either heads (H) or tails (T). Suppose we want to estimate the probability of getting heads (p) based on a series of coin flips. We can model this as a binomial distribution.\n",
    "\n",
    "Suppose we perform 10 coin flips and observe the following outcomes: H, T, H, H, T, T, H, H, H, T.\n",
    "\n",
    "The likelihood function represents the probability of observing this specific sequence of outcomes given different values of p. For example, if we assume that p = 0.5 (fair coin), the likelihood probability of observing the given sequence is calculated as:\n",
    "\n",
    "Likelihood probability = P(H, T, H, H, T, T, H, H, H, T | p = 0.5)\n",
    "\n",
    "For a binomial distribution, the likelihood can be calculated using the probability mass function (PMF) formula. In this case, the likelihood probability will depend on the specific sequence of outcomes and the assumed value of p.\n",
    "\n",
    "For example, if we assume a different value of p, such as p = 0.3, the likelihood probability will be:\n",
    "\n",
    "Likelihood probability = P(H, T, H, H, T, T, H, H, H, T | p = 0.3)\n",
    "\n",
    "The likelihood function provides a measure of how likely the observed data is under different parameter values. It is a key component in statistical inference and parameter estimation, where the goal is to find the parameter values that maximize the likelihood of the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab2a64",
   "metadata": {
    "id": "07ab2a64"
   },
   "source": [
    "##### 4. What is Naïve Bayes classifier ? Why is it named so ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ea164d",
   "metadata": {
    "id": "51ea164d"
   },
   "source": [
    "**Ans:** Naive Bayes is a simple and powerful algorithm for predictive modeling. Naive Bayes is called naive because it assumes that each input variable is independent. This is a strong assumption and unrealistic for real data; however, the technique is very effective on a large range of complex problems.\n",
    "\n",
    "\"\\\"\\\\\\\"![0_V0GyOt3LoDVfY7y5.png](https://miro.medium.com/proxy/1*ZW1icngckaSkivS0hXduIQ.jpeg)\\\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2162e",
   "metadata": {
    "id": "0be2162e"
   },
   "source": [
    "##### 5. What is optimal Bayes classifier ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7abae5",
   "metadata": {
    "id": "5b7abae5"
   },
   "source": [
    "**Ans:**\n",
    "The optimal Bayes classifier is based on Bayes' theorem and involves calculating the posterior probability of each class given the input features and selecting the class with the highest probability as the predicted label. It assumes that the true class probabilities and the conditional probabilities (likelihoods) are known.\n",
    "\n",
    "The process of the optimal Bayes classifier can be summarized as follows:\n",
    "\n",
    "Given an input instance with feature values, calculate the posterior probability of each class using Bayes' theorem.\n",
    "Select the class with the highest posterior probability as the predicted label for the instance.\n",
    "The optimal Bayes classifier aims to minimize the misclassification rate and provides the best achievable performance when the true class probabilities and conditional probabilities are known. However, in practice, the true probabilities are often unknown and need to be estimated from the available data using techniques such as maximum likelihood estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96163dd",
   "metadata": {
    "id": "a96163dd"
   },
   "source": [
    "##### 6. Write any two features of Bayesian learning methods ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09004071",
   "metadata": {
    "id": "09004071"
   },
   "source": [
    "**Ans:**\n",
    "\n",
    "**(1)Probabilistic Framework:** Bayesian learning methods are based on a probabilistic framework that enables modeling uncertainty. Instead of providing deterministic outputs, Bayesian methods assign probabilities to different outcomes, allowing for a more nuanced understanding of the data. These methods explicitly model the probability distributions of both the input data and the model parameters, allowing for the quantification of uncertainty in predictions and parameter estimates.\n",
    "\n",
    "**(2)Incorporation of Prior Knowledge:** Bayesian learning methods incorporate prior knowledge or beliefs about the model parameters into the learning process. Prior knowledge can be expressed in the form of prior distributions, which represent the initial beliefs about the parameters before seeing the data. This prior information is combined with the observed data to update the beliefs and obtain posterior distributions, which provide a refined estimate of the parameters. By incorporating prior knowledge, Bayesian methods can handle situations where limited data is available or where there is prior information about the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3b649e",
   "metadata": {
    "id": "6b3b649e"
   },
   "source": [
    "##### 7. Define the concept of consistent learners ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eba4091",
   "metadata": {
    "id": "6eba4091"
   },
   "source": [
    "**Ans:**\n",
    "**Consistent Learners:**\n",
    "In machine learning, a consistent learner refers to an algorithm or model that converges to the true underlying relationship or pattern in the data as the amount of training data increases. In other words, a consistent learner will produce increasingly accurate predictions or estimations as the size of the training dataset grows towards infinity.\n",
    "\n",
    "The concept of consistency is closely related to the notion of convergence in statistics. A consistent learner is designed to minimize the discrepancy between the learned model and the true model as more data becomes available. Consistency ensures that the learner's performance improves with larger and more representative datasets, leading to more reliable and accurate predictions.\n",
    "\n",
    "To formally define consistency, we can consider a sequence of training datasets of increasing size, denoted by D1, D2, D3, ..., where Di represents the ith training dataset. A learner is considered consistent if, as the size of the datasets increases, the learned model converges to the true underlying model in terms of the prediction or estimation.\n",
    "\n",
    "Consistency is a desirable property for machine learning algorithms as it provides theoretical guarantees that, given enough data, the learned model will approach the true model. However, it is important to note that the consistency of a learner depends on certain assumptions about the data and the learning algorithm. Violations of these assumptions, such as model misspecification or biased training data, can affect the consistency of the learner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54020cf8",
   "metadata": {
    "id": "54020cf8"
   },
   "source": [
    "##### 8. Write any two strengths of Bayes classifier ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28211a96",
   "metadata": {
    "id": "28211a96"
   },
   "source": [
    "**Ans:** This algorithm works quickly and can save a lot of time. Naive Bayes is suitable for solving multi-class prediction problems. If its assumption of the independence of features holds true, it can perform better than other models and requires much less training data.\n",
    "\n",
    "  * It is simple and easy to implement.\n",
    "  * It doesn't require as much training data.\n",
    "  * It handles both continuous and discrete data.\n",
    "  * It is highly scalable with the number of predictors and data points.\n",
    "  * It is fast and can be used to make real-time predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db681d1",
   "metadata": {
    "id": "9db681d1"
   },
   "source": [
    "##### 9. Write any two weaknesses of Bayes classifier ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7646cf4",
   "metadata": {
    "id": "d7646cf4"
   },
   "source": [
    "**Ans:** The greatest weakness of the naïve Bayes classifier is that it relies on an often-faulty assumption of equally important and independent features which results in biased posterior probabilities.\n",
    "\n",
    "\n",
    "  * If your test data set has a categorical variable of a category that wasn’t present in the training data set, the Naive Bayes model will assign it zero probability and won’t be able to make any predictions in this regard. This phenomenon is called ‘Zero Frequency,’ and you’ll have to use a smoothing technique to solve this problem.\n",
    "  * This algorithm is also notorious as a lousy estimator. So, you shouldn’t take the probability outputs of ‘predict_proba’ too seriously. \n",
    "  * It assumes that all the features are independent. While it might sound great in theory, in real life, you’ll hardly find a set of independent features. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b686929",
   "metadata": {
    "id": "5b686929"
   },
   "source": [
    "##### 10. Explain how Naïve Bayes classifier is used for:\n",
    "1. Text classification\n",
    "2. Spam filtering\n",
    "3. Market sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8447d36",
   "metadata": {
    "id": "d8447d36"
   },
   "source": [
    "**Ans:** Naive Bayes Classifier is used for:\n",
    "- **Text classification:**\n",
    "The Naive Bayes classifier is a simple classifier that classifies based on probabilities of events. It is the applied  commonly to text classification. With the training set, we can train a Naive Bayes classifier which we can use to automaticall categorize a new sentence.\n",
    "- **Spam filtering:**        \n",
    "Naive Bayes classifiers work by correlating the use of tokens (typically words, or sometimes other things), with spam and non-spam e-mails and then using Bayes' theorem to calculate a probability that an email is or is not spam.  It is one of the oldest ways of doing spam filtering, with roots in the 1990s.\n",
    "- **Market sentiment analysis:**    \n",
    "Market Sentiment analysis is a field dedicated to extracting subjective emotions and feelings from text. One common use of sentiment analysis is to figure out if a text expresses negative or positive feelings. Naive Bayes is a popular algorithm for classifying text."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ML_Assignment_12.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
